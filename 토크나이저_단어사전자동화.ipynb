{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO4uheUrB21rXAnar4W3Mcr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badro97/NLP_Practice/blob/main/%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80_%EB%8B%A8%EC%96%B4%EC%82%AC%EC%A0%84%EC%9E%90%EB%8F%99%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "XA8Rsu9mq5pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWnaXkJXVLYq"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy\n",
        "!pip install hgtk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토크나이저로 1차 분류 후 단어사전 단어 생성하기\n",
        "\n",
        "토크나이저는 Kkma 사용\n",
        "\n",
        "- 대용량 데이터 처리 속도가 가장 빠름"
      ],
      "metadata": {
        "id": "gBPveLV9Wd-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hgtk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Hannanum"
      ],
      "metadata": {
        "id": "V26HgmS2VUT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kkma =Kkma()\n",
        "okt = Okt()\n",
        "hannanum = Hannanum()"
      ],
      "metadata": {
        "id": "K-VVWPMIVmmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_jamo(token):\n",
        "    def to_special_token(jamo):\n",
        "      if not jamo:\n",
        "        return '-'\n",
        "      else:\n",
        "        return jamo\n",
        "    decomposed_token = ''\n",
        "    jong_list = []\n",
        "    for char in token:\n",
        "        try:\n",
        "        # char( 음 절 ) 을 초 성 , 중 성 , 종 성 으 로 분 리\n",
        "            cho, jung, jong = hgtk.letter.decompose(char)\n",
        "\n",
        "            # 자 모 가 빈 문 자 일 경 우 특 수 문 자 - 로 대 체\n",
        "            jong = to_special_token(jong)\n",
        "            jong_list.append(jong)\n",
        "              \n",
        "            # decomposed_token = decomposed_token + cho + jung + jong\n",
        "\n",
        "        # 만 약 char( 음 절 ) 이 한 글 이 아 닐 경 우 자 모 를 나 누 지 않 고 추 가\n",
        "        except Exception as exception:\n",
        "            if type(exception).__name__ == 'NotHangulException':\n",
        "                decomposed_token += char\n",
        "                \n",
        "    return jong_list"
      ],
      "metadata": {
        "id": "0dk7qvg9WaDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '해물왕해신짬뽕'"
      ],
      "metadata": {
        "id": "eOaVJYBQVoNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.morphs(text)"
      ],
      "metadata": {
        "id": "5RyhhTChVvaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 토큰마다 붙여보고 종성 기준 단어 생성\n",
        "\n",
        "2. 붙이는 건 한 개부터 ~ 하나 씩 늘려가며 전체를 다 붙일 때 까지 진행"
      ],
      "metadata": {
        "id": "_RXmpL28bSkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_collector(text):\n",
        "  token = kkma.morphs(text)\n",
        "  token_list = []\n",
        "  word_list = []\n",
        "  print(token)\n",
        "  for i in token:\n",
        "    for j in range(token.index(i), len(token)-1):\n",
        "      i += token[j+1]\n",
        "      jamo = word_to_jamo(i)\n",
        "      token_list.append(jamo)\n",
        "      word_list.append(i)\n",
        "  return token_list, word_list"
      ],
      "metadata": {
        "id": "ilM1WRkSYC03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_list, word_list = token_collector(text)"
      ],
      "metadata": {
        "id": "xjmaayXOZsH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_list"
      ],
      "metadata": {
        "id": "IVYgq5secrGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list"
      ],
      "metadata": {
        "id": "myWX9ZqbhRaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_list2, word_list2 = token_collector(\"소주칵테일\")"
      ],
      "metadata": {
        "id": "HGvrzCHhmSGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 종성 구분 규칙\n",
        "\n",
        "종성 유무로 단어 구분\n",
        "\n",
        "종성 없음 - \n",
        "\n",
        "종성 있음 ㅇ\n",
        "\n",
        "1. -ㅇ > -ㅇㅇㅇ- > -ㅇㅇㅇ\n",
        "2. -ㅇ- > -ㅇ-ㅇㅇ- > -ㅇ-ㅇㅇ\n",
        "3. ㅇㅇ > ㅇㅇ- - ㅇㅇ- > ㅇㅇ- (-ㅇㅇ(1))\n",
        "4. ㅇ- > ㅇ- -ㅇ > ㅇ- -"
      ],
      "metadata": {
        "id": "pYWxhtdcbZRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_check(title):\n",
        "  def token_collector(text):\n",
        "    token = kkma.morphs(text)\n",
        "    token_list = []\n",
        "    word_list = []\n",
        "    for i in token:\n",
        "      for j in range(token.index(i), len(token)-1):\n",
        "        i += token[j+1]\n",
        "        jamo = word_to_jamo(i)\n",
        "        token_list.append(jamo)\n",
        "        word_list.append(i)\n",
        "    return token, token_list, word_list\n",
        "\n",
        "  token, token_list, word_list = token_collector(title)\n",
        "  check = []\n",
        "  for tok, word in zip(token_list, word_list):\n",
        "    pos = np.where(np.array(tok) == '-')[0]\n",
        "    pos_len = len(pos)\n",
        "    # 종성이 하나만 없고 맨 앞이라면\n",
        "    if pos_len == 1 and pos[0] == 0:\n",
        "      check.append(word)\n",
        "    # 종성이 하나만 없다면\n",
        "    elif pos_len == 1:\n",
        "      check.append(word[:pos[0]])\n",
        "      check.append(word[pos[0]:])\n",
        "    # 종성이 여러개 없다면\n",
        "    elif pos_len != 1:\n",
        "      # 아예 종성이 없는 단어라면\n",
        "      if pos_len == len(token) and len(token) > 3:\n",
        "        check.append(word)\n",
        "        \n",
        "      # elif pos_len > 3:\n",
        "      #   check.append(word[pos[0]:])\n",
        "      #   for i in range(pos_len-1):\n",
        "      #     if pos[i+1] == pos[i]+1:\n",
        "      #       check.append(word[pos[i-1]:pos[i]])\n",
        "      #       check.append(word[pos[i]:pos[i+1]])\n",
        "      #       check.append(word[pos[i+1]:])\n",
        "            \n",
        "      #     else:\n",
        "      #       check.append(word[pos[0]:])\n",
        "  result = check + token\n",
        "  result = list(set(result))\n",
        "  result = [v for v in result if v]\n",
        "  return result"
      ],
      "metadata": {
        "id": "poKI1kJLbPIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_check('해물왕해신짬뽕')"
      ],
      "metadata": {
        "id": "z_yYBurknzGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_check('보쌈정식')"
      ],
      "metadata": {
        "id": "ML8Mq5iWo6CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_check('가마솥밥보쌈정식')"
      ],
      "metadata": {
        "id": "Ae_2fTQ0rG3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_check('후라이드오뎅탕')"
      ],
      "metadata": {
        "id": "DW_b13HktyUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.morphs('후라이드오뎅탕')"
      ],
      "metadata": {
        "id": "F7Z0oUfhrOWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/싼데비스탄/토큰수작업관련/back_token_count_1.csv', encoding='UTF-8', index_col=0)"
      ],
      "metadata": {
        "id": "WfP7qdx7o5zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = df['title']"
      ],
      "metadata": {
        "id": "rKNhUb6xwUq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for t in tqdm(title):\n",
        "  token = split_check(t)\n",
        "  tokens.append(token)\n",
        "tokens"
      ],
      "metadata": {
        "id": "2K5XK-lewZMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 막힌 부분\n",
        "\n",
        "종성없음 - 를 기준으로 로 슬라이싱 (종성 구분 규칙 구현)\n",
        "\n",
        "기존 토크나이저의 형태소 분류 문제\n",
        "\n",
        "## 결론\n",
        "\n",
        "수작업하러가자...ㅠ"
      ],
      "metadata": {
        "id": "oHqjYqxE6tCO"
      }
    }
  ]
}